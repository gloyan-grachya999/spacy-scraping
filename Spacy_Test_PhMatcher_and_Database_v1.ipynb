{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys #this probably isn't necessary, but i was using it so i could exit code to test it before it errored\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "#load nlp library via path\n",
    "nlp = spacy.load(r'en_core_web_sm') \n",
    "\n",
    "conn_string = (\n",
    "    r'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};'\n",
    "    r'DBQ=C:\\Users\\Muhammad Yasir\\Desktop\\KINS_Text_Test_11_Python_v1.accdb;')\n",
    "\n",
    "#connector to database\n",
    "conn = pyodbc.connect(conn_string)\n",
    "\n",
    "#pandas sql start\n",
    "sql='select name, uuid, short_description from Kins_Target_List_1 order by name'\n",
    "\n",
    "#create first datatable from Access DB\n",
    "dt = pd.read_sql(sql, conn)\n",
    "\n",
    "dt.head()\n",
    "\n",
    "display (dt)\n",
    "\n",
    "#dt['short_description']=dt['short_description'].apply(str)\n",
    "\n",
    "dt['tokenized_descriptions'] = dt['short_description'].apply(lambda x: list(nlp(x)))\n",
    "\n",
    "display (dt)\n",
    "\n",
    "\n",
    "#filter tokens\n",
    "def filter_stopwords(tokens):\n",
    "    return [token.text for token in tokens if token.is_alpha and not token.is_stop]\n",
    "dt['tokenized_descriptions_cleaned'] = dt['tokenized_descriptions'].apply(\n",
    "    lambda x: \n",
    "        list(filter_stopwords(x))\n",
    "            )\n",
    "\n",
    "display (dt)\n",
    "\n",
    "#convert to doc\n",
    "def make_doc(tokens):\n",
    "    return [nlp.make_doc(text) for text in tokens]\n",
    "dt['tokenized_descriptions_cleaned_asDoc'] = dt['tokenized_descriptions_cleaned'].apply(\n",
    "    lambda x:\n",
    "        list(make_doc(x))\n",
    "            )\n",
    "\n",
    "display (dt)\n",
    "\n",
    "#Starting with Matcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "#initialize the matcher with vocab\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "#define pattern for matching; THis should ultimately be pulled from the same database\n",
    "#my_lexicon = [{\"LOWER\": {\"IN\":[\"Saas\", \"Cloud\"]}}]\n",
    "#my_lexicon_terms = [\"saas\", \"cloud\", \"network\"]\n",
    "#my_lexicon_patterns = [nlp.make_doc(text) for text in my_lexicon_terms]\n",
    "\n",
    "#Reading from the database \"Lexicon\".\n",
    "\n",
    "query_terms = 'select Term from Lexicon'\n",
    "dt_terms = pd.read_sql(query_terms, conn)\n",
    "\n",
    "#Extracting terms and converting them to doc.\n",
    "lexicon_terms = dt_terms.Term.to_list()\n",
    "my_lexicon_patterns = [nlp.make_doc(text) for text in lexicon_terms]\n",
    "\n",
    "#define the token matcher\n",
    "matcher.add('Kins Lexicon Match', None, *my_lexicon_patterns)\n",
    "\n",
    "#run the matcher\n",
    "def piped_matcher(docs):\n",
    "    fin_result = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        end_result = []\n",
    "        character_matches = matcher(doc)\n",
    "        for match_id, start, end in character_matches:\n",
    "            span = doc[start:end]\n",
    "            end_result.append(span)\n",
    "        fin_result.append(end_result)\n",
    "    return fin_result\n",
    "\n",
    "dt['Matches'] = dt['tokenized_descriptions_cleaned_asDoc'].apply(\n",
    "    lambda x: \n",
    "        piped_matcher(x)\n",
    ")      \n",
    "display(dt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
